SCRAPTOR.

http://scrapinghub.com/platform/

Scraptor should be able to scrape website providing basic funcitonality such as the following that I implement in all of my projects.

Debug decorator.
 A decorator that provides error handling with text messaging.
A command line interface that allows you to set up your project with password if you want to use the command line interface.
This would ensure that the username and the password is encoded with a local ssh-key so that nobody can ever see it in plain text.
Continous deployment even as a deamon.
Options to connect to firebase. Again encrypting in ssh-key the secret key. This is by using my common component library.

You could do everything through command line or you could programatically use it.
The command line would write python code. Python code with motherfucking comments.

The scraper could save the information to firebase as a dafault. To a database. To a text file.

One of the main problems that I have is that I often save the same information twice. Therefore a sort of key field must be created. Using a simple presence algorithm.

I also have to have as less dependencies as possible. For example requests and maybe, just maybe selenium because it fucking works.

How could I fucking go as low level as possible. I have no idea. How do people make modules that run by themselves I have no idea.
Is multithreading possibly using this primitive python library. I also have no idea.

How is this superios that other scrapping modules for python. Well I will also try to provide a metaclassing interface.
My python script assumes at least three things.

1. The scrapper can be distributed.
2. The scrapper can run all the time.
3. The scrapper can stop at some point and return in another point.
4. You could use the generated python code or you could use your own code.


Therefore the python script should be divided in a couple of common modules.

This may be my first major framework design.

My script should never reflect this file.

What are the advantages of my system over other systems. Why companies would use my system over other systems.

My system has to reflect basic architectural principles of software design. 

It has to be open to the open source community. It has to be able to scrape goverment websites. Listing directories. Netflix. Craigslist.

There also needs to be a machanism to wait for responses therefore the main scraping module should be using selenium because it is possible that the scripts load content through ajax calls. People do not want to insepct ajax calls one by one therefore my system must provide waiting mechanisms.

What about a graphing interface in realtime of the results of the srapers that strams data obtained by the scrapers
